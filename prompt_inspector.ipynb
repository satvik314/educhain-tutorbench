{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ” Prompt Inspector\n",
    "\n",
    "This notebook provides an interactive way to inspect prompts, model responses, user feedback, and LLM evaluations from the database.\n",
    "\n",
    "## Features:\n",
    "- ğŸ“‹ View complete prompt information\n",
    "- ğŸ¤– See all model responses\n",
    "- â­ Check user ratings and feedback\n",
    "- ğŸ§  Review LLM evaluations\n",
    "- ğŸ•’ List recent prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client, Client\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Imports loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptInspector Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PromptInspector initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "class PromptInspector:\n",
    "    def __init__(self):\n",
    "        self.supabase_url = os.getenv(\"SUPABASE_URL\")\n",
    "        self.supabase_key = os.getenv(\"SUPABASE_ANON_KEY\")\n",
    "        \n",
    "        if not self.supabase_url or not self.supabase_key:\n",
    "            raise ValueError(\"Missing SUPABASE_URL or SUPABASE_ANON_KEY environment variables\")\n",
    "        \n",
    "        self.supabase: Client = create_client(self.supabase_url, self.supabase_key)\n",
    "    \n",
    "    def get_prompt_info(self, prompt_id=None, prompt_text=None):\n",
    "        \"\"\"Retrieve complete prompt information\"\"\"\n",
    "        \n",
    "        if prompt_id:\n",
    "            # Get by prompt ID\n",
    "            prompt_result = self.supabase.table(\"prompts\").select(\"*\").eq(\"id\", prompt_id).execute()\n",
    "            if not prompt_result.data:\n",
    "                print(f\"âŒ No prompt found with ID: {prompt_id}\")\n",
    "                return None\n",
    "            prompt_data = prompt_result.data[0]\n",
    "            \n",
    "        elif prompt_text:\n",
    "            # Get most recent prompt matching the text\n",
    "            prompt_result = self.supabase.table(\"prompts\")\\\n",
    "                .select(\"*\")\\\n",
    "                .eq(\"prompt_text\", prompt_text)\\\n",
    "                .order(\"created_at\", desc=True)\\\n",
    "                .limit(1)\\\n",
    "                .execute()\n",
    "            if not prompt_result.data:\n",
    "                print(f\"âŒ No prompt found with text: {prompt_text}\")\n",
    "                return None\n",
    "            prompt_data = prompt_result.data[0]\n",
    "            prompt_id = prompt_data[\"id\"]\n",
    "        else:\n",
    "            print(\"âŒ Please provide either prompt_id or prompt_text\")\n",
    "            return None\n",
    "        \n",
    "        # Get all responses for this prompt\n",
    "        responses_result = self.supabase.table(\"model_responses\")\\\n",
    "            .select(\"*\")\\\n",
    "            .eq(\"prompt_id\", prompt_id)\\\n",
    "            .order(\"created_at\")\\\n",
    "            .execute()\n",
    "        \n",
    "        # Get feedback for this prompt\n",
    "        feedback_result = self.supabase.table(\"response_feedback\")\\\n",
    "            .select(\"*\")\\\n",
    "            .eq(\"prompt_id\", prompt_id)\\\n",
    "            .execute()\n",
    "        \n",
    "        # Get LLM evaluations if they exist\n",
    "        evaluations_result = self.supabase.table(\"llm_evaluations\")\\\n",
    "            .select(\"*\")\\\n",
    "            .eq(\"prompt_id\", prompt_id)\\\n",
    "            .execute()\n",
    "        \n",
    "        return {\n",
    "            'prompt': prompt_data,\n",
    "            'responses': responses_result.data,\n",
    "            'feedback': feedback_result.data,\n",
    "            'evaluations': evaluations_result.data\n",
    "        }\n",
    "    \n",
    "    def display_prompt_info(self, info):\n",
    "        \"\"\"Display prompt information in a formatted way\"\"\"\n",
    "        if not info:\n",
    "            return\n",
    "        \n",
    "        prompt = info['prompt']\n",
    "        responses = info['responses']\n",
    "        feedback = info['feedback']\n",
    "        evaluations = info['evaluations']\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"ğŸ“‹ PROMPT INFORMATION\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Prompt details\n",
    "        print(f\"ğŸ†” ID: {prompt['id']}\")\n",
    "        print(f\"ğŸ‘¤ Username: {prompt.get('username', 'Anonymous')}\")\n",
    "        print(f\"ğŸ“… Created: {self._format_datetime(prompt['created_at'])}\")\n",
    "        print(f\"ğŸ“Š Status: {prompt['status']}\")\n",
    "        print(f\"ğŸ”¢ Total Models: {prompt['total_models']}\")\n",
    "        print(f\"ğŸ¯ Selected Models: {', '.join(prompt['selected_models'])}\")\n",
    "        print()\n",
    "        print(f\"ğŸ’¬ Prompt Text:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(prompt['prompt_text'])\n",
    "        print(\"-\" * 40)\n",
    "        print()\n",
    "        \n",
    "        # Responses\n",
    "        print(\"ğŸ¤– MODEL RESPONSES\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for i, response in enumerate(responses, 1):\n",
    "            print(f\"\\n{i}. {response['model_name']}\")\n",
    "            print(f\"   â±ï¸ Response Time: {response.get('response_time_ms', 'N/A')} ms\")\n",
    "            print(f\"   ğŸ“… Created: {self._format_datetime(response['created_at'])}\")\n",
    "            \n",
    "            if response['response_error']:\n",
    "                print(f\"   âŒ Error: {response['response_error']}\")\n",
    "            else:\n",
    "                content = response['response_content']\n",
    "                if len(content) > 200:\n",
    "                    print(f\"   ğŸ“ Content (first 200 chars): {content[:200]}...\")\n",
    "                else:\n",
    "                    print(f\"   ğŸ“ Content: {content}\")\n",
    "            print()\n",
    "        \n",
    "        # Feedback\n",
    "        if feedback:\n",
    "            print(\"ğŸ’­ USER FEEDBACK\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            feedback_by_response = {}\n",
    "            for fb in feedback:\n",
    "                response_id = fb['response_id']\n",
    "                if response_id not in feedback_by_response:\n",
    "                    feedback_by_response[response_id] = []\n",
    "                feedback_by_response[response_id].append(fb)\n",
    "            \n",
    "            for response in responses:\n",
    "                response_id = response['id']\n",
    "                model_name = response['model_name']\n",
    "                \n",
    "                if response_id in feedback_by_response:\n",
    "                    print(f\"\\nğŸ¤– {model_name}\")\n",
    "                    for fb in feedback_by_response[response_id]:\n",
    "                        print(f\"   ğŸ‘¤ User: {fb.get('username', 'Anonymous')}\")\n",
    "                        if fb['rating']:\n",
    "                            stars = \"â­\" * fb['rating']\n",
    "                            print(f\"   â­ Rating: {stars} ({fb['rating']}/5)\")\n",
    "                        if fb['rank_position']:\n",
    "                            rank_emoji = \"ğŸ¥‡\" if fb['rank_position'] == 1 else \"ğŸ¥ˆ\" if fb['rank_position'] == 2 else \"ğŸ¥‰\" if fb['rank_position'] == 3 else \"ğŸ…\"\n",
    "                            print(f\"   ğŸ† Rank: #{fb['rank_position']} {rank_emoji}\")\n",
    "                        if fb['feedback_text']:\n",
    "                            print(f\"   ğŸ’¬ Comment: {fb['feedback_text']}\")\n",
    "                        print(f\"   ğŸ“… Given: {self._format_datetime(fb['created_at'])}\")\n",
    "                        print()\n",
    "        else:\n",
    "            print(\"\\nğŸ’­ USER FEEDBACK: None\")\n",
    "        \n",
    "        # LLM Evaluations\n",
    "        if evaluations:\n",
    "            print(\"\\nğŸ§  LLM EVALUATIONS\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            for eval_data in evaluations:\n",
    "                print(f\"\\nğŸ¤– {eval_data['model_name']}\")\n",
    "                print(f\"   ğŸ§‘â€âš–ï¸ Judge: {eval_data['judge_model']}\")\n",
    "                print(f\"   ğŸ“… Evaluated: {self._format_datetime(eval_data['created_at'])}\")\n",
    "                \n",
    "                if eval_data['scores']:\n",
    "                    scores = json.loads(eval_data['scores']) if isinstance(eval_data['scores'], str) else eval_data['scores']\n",
    "                    print(f\"   ğŸ“Š Scores:\")\n",
    "                    if 'overall' in scores:\n",
    "                        print(f\"      Overall: {scores['overall']}/10\")\n",
    "                    if 'confusion_recognition' in scores:\n",
    "                        print(f\"      Confusion Recognition: {scores['confusion_recognition']}/10\")\n",
    "                    if 'adaptive_response' in scores:\n",
    "                        print(f\"      Adaptive Response: {scores['adaptive_response']}/10\")\n",
    "                    if 'learning_facilitation' in scores:\n",
    "                        print(f\"      Learning Facilitation: {scores['learning_facilitation']}/10\")\n",
    "                    if 'strategic_decision' in scores:\n",
    "                        print(f\"      Strategic Decision: {scores['strategic_decision']}/10\")\n",
    "                    if 'engagement_eq' in scores:\n",
    "                        print(f\"      Engagement & EQ: {scores['engagement_eq']}/10\")\n",
    "                \n",
    "                # Show first 300 chars of evaluation\n",
    "                eval_text = eval_data['evaluation_text']\n",
    "                if len(eval_text) > 300:\n",
    "                    print(f\"   ğŸ“ Evaluation (first 300 chars): {eval_text[:300]}...\")\n",
    "                else:\n",
    "                    print(f\"   ğŸ“ Evaluation: {eval_text}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"\\nğŸ§  LLM EVALUATIONS: None\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    def _format_datetime(self, dt_string):\n",
    "        \"\"\"Format datetime string for display\"\"\"\n",
    "        try:\n",
    "            dt = datetime.fromisoformat(dt_string.replace('Z', '+00:00'))\n",
    "            return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        except:\n",
    "            return dt_string\n",
    "    \n",
    "    def list_recent_prompts(self, limit=10):\n",
    "        \"\"\"List recent prompts\"\"\"\n",
    "        result = self.supabase.table(\"prompts\")\\\n",
    "            .select(\"id, username, prompt_text, created_at, total_models\")\\\n",
    "            .order(\"created_at\", desc=True)\\\n",
    "            .limit(limit)\\\n",
    "            .execute()\n",
    "        \n",
    "        print(\"ğŸ•’ RECENT PROMPTS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for i, prompt in enumerate(result.data, 1):\n",
    "            prompt_preview = prompt['prompt_text'][:60] + \"...\" if len(prompt['prompt_text']) > 60 else prompt['prompt_text']\n",
    "            print(f\"{i}. ID: {prompt['id']}\")\n",
    "            print(f\"   ğŸ‘¤ {prompt.get('username', 'Anonymous')} | ğŸ¤– {prompt['total_models']} models | ğŸ“… {self._format_datetime(prompt['created_at'])}\")\n",
    "            print(f\"   ğŸ’¬ {prompt_preview}\")\n",
    "            print()\n",
    "    \n",
    "    def get_responses_dataframe(self, prompt_id):\n",
    "        \"\"\"Get responses as a pandas DataFrame for easy analysis\"\"\"\n",
    "        info = self.get_prompt_info(prompt_id=prompt_id)\n",
    "        if not info or not info['responses']:\n",
    "            return None\n",
    "        \n",
    "        responses_data = []\n",
    "        for response in info['responses']:\n",
    "            responses_data.append({\n",
    "                'model_name': response['model_name'],\n",
    "                'response_time_ms': response.get('response_time_ms'),\n",
    "                'has_error': bool(response['response_error']),\n",
    "                'content_length': len(response['response_content']) if response['response_content'] else 0,\n",
    "                'created_at': response['created_at']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(responses_data)\n",
    "\n",
    "# Initialize the inspector\n",
    "inspector = PromptInspector()\n",
    "print(\"âœ… PromptInspector initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ•’ List Recent Prompts\n",
    "\n",
    "Get an overview of recent prompts to find the one you want to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ•’ RECENT PROMPTS\n",
      "============================================================\n",
      "1. ID: 965c4175-1e48-4cd5-a4d5-af229ffee8b4\n",
      "   ğŸ‘¤ None | ğŸ¤– 4 models | ğŸ“… 2025-09-27 02:56:29\n",
      "   ğŸ’¬ \n",
      "I am writing a fun educational article on 'Matrices and Gen...\n",
      "\n",
      "2. ID: 1b7442da-f930-42a3-ab8b-4a7d457ee648\n",
      "   ğŸ‘¤ Satvik | ğŸ¤– 4 models | ğŸ“… 2025-09-27 01:17:15\n",
      "   ğŸ’¬ Explain transformers architecture like I am 16. Use basic te...\n",
      "\n",
      "3. ID: 27a38909-e520-4a65-8663-8fbae0a4ded4\n",
      "   ğŸ‘¤ Satvik | ğŸ¤– 3 models | ğŸ“… 2025-09-26 08:38:51\n",
      "   ğŸ’¬ Student: \"I simplified xÂ² + xÂ³ and got xâµ because 2 + 3 = 5\"...\n",
      "\n",
      "4. ID: c9fc23c4-d7c5-4df4-ac2f-20ad4a424b58\n",
      "   ğŸ‘¤ Satvik | ğŸ¤– 3 models | ğŸ“… 2025-09-25 10:33:11\n",
      "   ğŸ’¬ A student has made the same mistake 5 times even after your ...\n",
      "\n",
      "5. ID: 9b26b181-bb4f-4270-8a62-99b14f5c9c90\n",
      "   ğŸ‘¤ None | ğŸ¤– 3 models | ğŸ“… 2025-09-25 09:26:54\n",
      "   ğŸ’¬ Student: \"I keep getting different answers when I solve 2x +...\n",
      "\n",
      "6. ID: 836d3ed0-8be0-4b92-9535-d9dcfd40706b\n",
      "   ğŸ‘¤ None | ğŸ¤– 4 models | ğŸ“… 2025-09-25 09:12:55\n",
      "   ğŸ’¬ Student: \"Heavy objects fall faster because gravity pulls th...\n",
      "\n",
      "7. ID: 39b3e194-2444-4bc4-9f6a-2498112cf509\n",
      "   ğŸ‘¤ Satvik | ğŸ¤– 3 models | ğŸ“… 2025-09-25 08:54:31\n",
      "   ğŸ’¬ Create a scenario based question to test understanding of Ne...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List the 10 most recent prompts\n",
    "inspector.list_recent_prompts(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Inspect Specific Prompt\n",
    "\n",
    "Choose one of the methods below to inspect a specific prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: By Prompt ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“‹ PROMPT INFORMATION\n",
      "================================================================================\n",
      "ğŸ†” ID: 1b7442da-f930-42a3-ab8b-4a7d457ee648\n",
      "ğŸ‘¤ Username: Satvik\n",
      "ğŸ“… Created: 2025-09-27 01:17:15\n",
      "ğŸ“Š Status: completed\n",
      "ğŸ”¢ Total Models: 4\n",
      "ğŸ¯ Selected Models: openai/gpt-5, anthropic/claude-sonnet-4, deepseek/deepseek-v3.1-terminus, moonshotai/kimi-k2-0905\n",
      "\n",
      "ğŸ’¬ Prompt Text:\n",
      "----------------------------------------\n",
      "Explain transformers architecture like I am 16. Use basic terms of architecture like encoder, decoder, self-attention, parallelization\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ¤– MODEL RESPONSES\n",
      "==================================================\n",
      "\n",
      "1. openai/gpt-5\n",
      "   â±ï¸ Response Time: 6986 ms\n",
      "   ğŸ“… Created: 2025-09-27 01:17:43\n",
      "   ğŸ“ Content (first 200 chars): Hereâ€™s the transformer idea in plain terms, using the key parts you named.\n",
      "\n",
      "What problem it solves\n",
      "- We want a model that can read a sequence (like a sentence) and write a sequence (like a translation...\n",
      "\n",
      "\n",
      "2. anthropic/claude-sonnet-4\n",
      "   â±ï¸ Response Time: 7041 ms\n",
      "   ğŸ“… Created: 2025-09-27T01:17:43.95074+00:00\n",
      "   ğŸ“ Content (first 200 chars): # Transformers: The Brain Behind ChatGPT and Modern AI\n",
      "\n",
      "Imagine you're trying to understand a really long text message from your friend. Your brain doesn't just read word by word - it connects differe...\n",
      "\n",
      "\n",
      "3. deepseek/deepseek-v3.1-terminus\n",
      "   â±ï¸ Response Time: 7068 ms\n",
      "   ğŸ“… Created: 2025-09-27 01:17:44\n",
      "   ğŸ“ Content (first 200 chars): Of course! Let's break down Transformers like you're building a super-smart, super-fast translation machine.\n",
      "\n",
      "Imagine you have two friends: **Alex the Architect** and **Dana the Decoder**. They need t...\n",
      "\n",
      "\n",
      "4. moonshotai/kimi-k2-0905\n",
      "   â±ï¸ Response Time: 7103 ms\n",
      "   ğŸ“… Created: 2025-09-27 01:17:44\n",
      "   ğŸ“ Content (first 200 chars): Imagine you and your best friend want to send secret messages across the classroom, but you can only pass folded notes.  \n",
      "The â€œTransformerâ€ is the super-smart note-passing system that lets you do it f...\n",
      "\n",
      "ğŸ’­ USER FEEDBACK\n",
      "==================================================\n",
      "\n",
      "ğŸ¤– anthropic/claude-sonnet-4\n",
      "   ğŸ‘¤ User: Satvik\n",
      "   â­ Rating: â­â­â­â­ (4/5)\n",
      "   ğŸ“… Given: 2025-09-27 01:31:05\n",
      "\n",
      "\n",
      "ğŸ¤– deepseek/deepseek-v3.1-terminus\n",
      "   ğŸ‘¤ User: Satvik\n",
      "   â­ Rating: â­â­â­ (3/5)\n",
      "   ğŸ“… Given: 2025-09-27 01:31:05\n",
      "\n",
      "\n",
      "ğŸ¤– moonshotai/kimi-k2-0905\n",
      "   ğŸ‘¤ User: Satvik\n",
      "   â­ Rating: â­ (1/5)\n",
      "   ğŸ“… Given: 2025-09-27 01:31:06\n",
      "\n",
      "\n",
      "ğŸ§  LLM EVALUATIONS\n",
      "==================================================\n",
      "\n",
      "ğŸ¤– openai/gpt-5\n",
      "   ğŸ§‘â€âš–ï¸ Judge: x-ai/grok-4-fast:free\n",
      "   ğŸ“… Evaluated: 2025-09-27 19:46:46\n",
      "   ğŸ“ Evaluation: Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "\n",
      "\n",
      "ğŸ¤– anthropic/claude-sonnet-4\n",
      "   ğŸ§‘â€âš–ï¸ Judge: x-ai/grok-4-fast:free\n",
      "   ğŸ“… Evaluated: 2025-09-27 19:46:46\n",
      "   ğŸ“ Evaluation: Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "\n",
      "\n",
      "ğŸ¤– deepseek/deepseek-v3.1-terminus\n",
      "   ğŸ§‘â€âš–ï¸ Judge: x-ai/grok-4-fast:free\n",
      "   ğŸ“… Evaluated: 2025-09-27 19:46:47\n",
      "   ğŸ“ Evaluation: Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "\n",
      "\n",
      "ğŸ¤– moonshotai/kimi-k2-0905\n",
      "   ğŸ§‘â€âš–ï¸ Judge: x-ai/grok-4-fast:free\n",
      "   ğŸ“… Evaluated: 2025-09-27T19:46:47.58212+00:00\n",
      "   ğŸ“ Evaluation: Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "\n",
      "\n",
      "ğŸ¤– openai/gpt-5\n",
      "   ğŸ§‘â€âš–ï¸ Judge: x-ai/grok-4-fast:free\n",
      "   ğŸ“… Evaluated: 2025-09-27 19:50:38\n",
      "   ğŸ“ Evaluation: Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "\n",
      "\n",
      "ğŸ¤– anthropic/claude-sonnet-4\n",
      "   ğŸ§‘â€âš–ï¸ Judge: x-ai/grok-4-fast:free\n",
      "   ğŸ“… Evaluated: 2025-09-27 19:50:39\n",
      "   ğŸ“ Evaluation: Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "\n",
      "\n",
      "ğŸ¤– deepseek/deepseek-v3.1-terminus\n",
      "   ğŸ§‘â€âš–ï¸ Judge: x-ai/grok-4-fast:free\n",
      "   ğŸ“… Evaluated: 2025-09-27 19:50:39\n",
      "   ğŸ“ Evaluation: Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "\n",
      "\n",
      "ğŸ¤– moonshotai/kimi-k2-0905\n",
      "   ğŸ§‘â€âš–ï¸ Judge: x-ai/grok-4-fast:free\n",
      "   ğŸ“… Evaluated: 2025-09-27 19:50:40\n",
      "   ğŸ“ Evaluation: Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Replace with your actual prompt ID\n",
    "prompt_id = \"1b7442da-f930-42a3-ab8b-4a7d457ee648\"\n",
    "\n",
    "info = inspector.get_prompt_info(prompt_id=prompt_id)\n",
    "inspector.display_prompt_info(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: By Prompt Text (finds most recent match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ No prompt found with text: Explain transformers architecture like I am 16\n"
     ]
    }
   ],
   "source": [
    "# Replace with your actual prompt text\n",
    "prompt_text = \"Explain transformers architecture like I am 16\"\n",
    "\n",
    "info = inspector.get_prompt_info(prompt_text=prompt_text)\n",
    "inspector.display_prompt_info(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Response Analysis DataFrame\n",
    "\n",
    "Get a pandas DataFrame for easy analysis of response metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Response Metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>response_time_ms</th>\n",
       "      <th>has_error</th>\n",
       "      <th>content_length</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>openai/gpt-5</td>\n",
       "      <td>6986</td>\n",
       "      <td>False</td>\n",
       "      <td>4105</td>\n",
       "      <td>2025-09-27T01:17:43.792374+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anthropic/claude-sonnet-4</td>\n",
       "      <td>7041</td>\n",
       "      <td>False</td>\n",
       "      <td>3430</td>\n",
       "      <td>2025-09-27T01:17:43.95074+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deepseek/deepseek-v3.1-terminus</td>\n",
       "      <td>7068</td>\n",
       "      <td>False</td>\n",
       "      <td>4775</td>\n",
       "      <td>2025-09-27T01:17:44.070435+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>moonshotai/kimi-k2-0905</td>\n",
       "      <td>7103</td>\n",
       "      <td>False</td>\n",
       "      <td>2106</td>\n",
       "      <td>2025-09-27T01:17:44.197266+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model_name  response_time_ms  has_error  \\\n",
       "0                     openai/gpt-5              6986      False   \n",
       "1        anthropic/claude-sonnet-4              7041      False   \n",
       "2  deepseek/deepseek-v3.1-terminus              7068      False   \n",
       "3          moonshotai/kimi-k2-0905              7103      False   \n",
       "\n",
       "   content_length                        created_at  \n",
       "0            4105  2025-09-27T01:17:43.792374+00:00  \n",
       "1            3430   2025-09-27T01:17:43.95074+00:00  \n",
       "2            4775  2025-09-27T01:17:44.070435+00:00  \n",
       "3            2106  2025-09-27T01:17:44.197266+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ Summary Statistics:\n",
      "Average response time: 7049.5 ms\n",
      "Fastest model: openai/gpt-5\n",
      "Slowest model: moonshotai/kimi-k2-0905\n",
      "Average content length: 3604 characters\n",
      "Models with errors: 0\n"
     ]
    }
   ],
   "source": [
    "# Get responses as DataFrame\n",
    "prompt_id = \"1b7442da-f930-42a3-ab8b-4a7d457ee648\"  # Replace with your prompt ID\n",
    "\n",
    "df = inspector.get_responses_dataframe(prompt_id)\n",
    "if df is not None:\n",
    "    print(\"ğŸ“Š Response Metrics:\")\n",
    "    display(df)\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ Summary Statistics:\")\n",
    "    print(f\"Average response time: {df['response_time_ms'].mean():.1f} ms\")\n",
    "    print(f\"Fastest model: {df.loc[df['response_time_ms'].idxmin(), 'model_name']}\")\n",
    "    print(f\"Slowest model: {df.loc[df['response_time_ms'].idxmax(), 'model_name']}\")\n",
    "    print(f\"Average content length: {df['content_length'].mean():.0f} characters\")\n",
    "    print(f\"Models with errors: {df['has_error'].sum()}\")\n",
    "else:\n",
    "    print(\"âŒ No response data found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  Extract Full Response Content\n",
    "\n",
    "Get the full response content for detailed analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ PROMPT: Explain transformers architecture like I am 16. Use basic terms of architecture like encoder, decoder, self-attention, parallelization\n",
      "================================================================================\n",
      "\n",
      "ğŸ¤– 1. openai/gpt-5\n",
      "--------------------------------------------------\n",
      "Hereâ€™s the transformer idea in plain terms, using the key parts you named.\n",
      "\n",
      "What problem it solves\n",
      "- We want a model that can read a sequence (like a sentence) and write a sequence (like a translation or the next words), while noticing relationships between words far apart. We also want it to run fast on modern hardware.\n",
      "\n",
      "Core pieces\n",
      "- Tokens and embeddings: Words (or word-pieces) are turned into numbers called embeddings so the model can do math with them.\n",
      "- Positional encoding: Since a bag of words ignores order, we add position info so the model knows â€œthis is the 3rd word,â€ â€œthis is the 4th,â€ etc.\n",
      "- Self-attention: For each word, the model looks at all other words and decides how much to pay attention to each. It then blends information from the most relevant words. Think: â€œWhen understanding â€˜bank,â€™ should I focus on â€˜riverâ€™ or â€˜moneyâ€™ in this sentence?â€\n",
      "- Multi-head attention: The model does several attention lookups in parallel (â€œheadsâ€). Each head can focus on a different kind of relationship (like grammar vs. meaning), then their results are combined.\n",
      "- Feed-forward layer: After attention mixes info across words, a small neural network refines each wordâ€™s representation individually.\n",
      "- Residual connections and layer norm: Simple wiring that stabilizes and speeds up training by keeping signals from vanishing or exploding.\n",
      "\n",
      "Encoderâ€“decoder layout\n",
      "- Encoder: Reads the input sequence.\n",
      "  - Each encoder layer = self-attention (everyone looks at everyone) + feed-forward.\n",
      "  - Stacking many layers lets the model build rich, global understanding of the input.\n",
      "- Decoder: Writes the output sequence step by step.\n",
      "  - Masked self-attention: When generating, each position can only look left (past words), not right (future words), so it canâ€™t â€œcheat.â€\n",
      "  - Cross-attention: The decoder also looks at the encoderâ€™s output to pick the most relevant parts of the input.\n",
      "  - Feed-forward after attention, same as encoder.\n",
      "\n",
      "Parallelization (why itâ€™s fast)\n",
      "- In self-attention, all positions can be processed at once. This matches GPUs/TPUs perfectly.\n",
      "- Old RNNs read one word at a time in order. Transformers process a whole sentence in parallel during training, which is much faster and scales better.\n",
      "\n",
      "How a translation model (encoderâ€“decoder) runs\n",
      "1) Input sentence â†’ tokenize â†’ embeddings + positions.\n",
      "2) Encoder layers apply self-attention to understand the whole sentence.\n",
      "3) Decoder starts generating:\n",
      "   - For the first output token, masked self-attention looks at whatâ€™s already generated (nothing at first).\n",
      "   - Cross-attention looks at the encoderâ€™s output to find relevant input words.\n",
      "   - It predicts the first token. Then repeats for the second, third, etc., each time seeing more of its own past outputs.\n",
      "4) At training time, we compare predictions to the correct target sentence and update the model. At inference, we choose tokens via strategies like greedy, sampling, or beam search.\n",
      "\n",
      "Why self-attention helps\n",
      "- Long-range relationships: Attention can connect distant words in one step (RNNs often struggle here).\n",
      "- Flexibility: Different heads learn different patterns.\n",
      "- Interpretability: Attention weights sometimes show what the model focused on.\n",
      "\n",
      "Common variants\n",
      "- Encoder-only (BERT): Great for reading/understanding tasks (classification, search). No decoder.\n",
      "- Decoder-only (GPT): Great for generating text (next-token prediction).\n",
      "- Encoderâ€“decoder (T5, original Transformer): Great for tasks like translation or summarization framed as input â†’ output text.\n",
      "\n",
      "Limits and trade-offs\n",
      "- Cost grows with sequence length (attention over all pairs of tokens).\n",
      "- Context window is finite (only so many tokens at once).\n",
      "- Many improvements and â€œefficient attentionâ€ tricks exist, but the core ideas above stay the same.\n",
      "\n",
      "One-sentence cheat sheet\n",
      "- Encoder: Understand the input with self-attention.\n",
      "- Decoder: Generate the output with masked self-attention and cross-attention to the encoder.\n",
      "- Self-attention: Each word looks at all others to decide what matters.\n",
      "- Parallelization: The model processes many tokens at the same time, making training fast and scalable.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ¤– 2. anthropic/claude-sonnet-4\n",
      "--------------------------------------------------\n",
      "# Transformers: The Brain Behind ChatGPT and Modern AI\n",
      "\n",
      "Imagine you're trying to understand a really long text message from your friend. Your brain doesn't just read word by word - it connects different parts of the message, remembers important context, and figures out what everything means together. Transformers work similarly!\n",
      "\n",
      "## What is a Transformer?\n",
      "\n",
      "A transformer is like a super-smart reading comprehension system. It's the architecture (blueprint) behind ChatGPT, Google Translate, and most modern AI language models. Think of it as having two main parts working together:\n",
      "\n",
      "## ğŸ” **Encoder: The \"Reader\"**\n",
      "\n",
      "The encoder is like a really good student who reads and understands text:\n",
      "\n",
      "- **What it does**: Takes input text (like \"How are you today?\") and creates a deep understanding of what each word means in context\n",
      "- **Think of it like**: A student highlighting and taking detailed notes about every important part of a passage\n",
      "- **Real example**: When you type \"bank\" - the encoder figures out if you mean a river bank or a money bank based on the other words around it\n",
      "\n",
      "## ğŸ¯ **Decoder: The \"Writer\"**\n",
      "\n",
      "The decoder is like a creative writer who uses the encoder's notes to generate responses:\n",
      "\n",
      "- **What it does**: Takes the encoder's understanding and generates new text word by word\n",
      "- **Think of it like**: A student using their notes to write an essay or answer questions\n",
      "- **Real example**: After understanding your question about banks, it generates a relevant response\n",
      "\n",
      "## ğŸ§  **Self-Attention: The \"Connection Maker\"**\n",
      "\n",
      "This is the secret sauce! Self-attention is like having a superpower to instantly connect related ideas:\n",
      "\n",
      "- **What it does**: Looks at every word and asks \"Which other words in this sentence are important for understanding THIS word?\"\n",
      "- **Think of it like**: Your brain automatically linking \"it\" back to what \"it\" refers to, or connecting \"tall\" with \"building\" even if they're far apart in a sentence\n",
      "- **Cool example**: In \"The cat sat on the mat because it was comfortable\" - self-attention connects \"it\" back to either \"cat\" or \"mat\" based on context\n",
      "\n",
      "## âš¡ **Parallelization: The \"Speed Boost\"**\n",
      "\n",
      "This is what makes transformers lightning fast:\n",
      "\n",
      "- **Old way**: Like reading a book word by word, one at a time (slow!)\n",
      "- **Transformer way**: Like having 100 friends each read different parts of the book simultaneously, then sharing notes (super fast!)\n",
      "- **Why it matters**: Instead of processing \"How are you today?\" one word at a time, it can analyze all words at once\n",
      "\n",
      "## Putting It All Together\n",
      "\n",
      "Here's how it works when you chat with ChatGPT:\n",
      "\n",
      "1. **You type**: \"What's the weather like?\"\n",
      "2. **Encoder**: Reads your question, uses self-attention to understand that \"weather\" and \"like\" go together to mean you want a description\n",
      "3. **Decoder**: Generates a response word by word: \"I\" â†’ \"don't\" â†’ \"have\" â†’ \"access\" â†’ \"to\" â†’ \"current\" â†’ \"weather\"\n",
      "4. **Parallelization**: All of this happens super fast because multiple processes work simultaneously\n",
      "\n",
      "## Why Transformers Are Game-Changers\n",
      "\n",
      "- **Better understanding**: Self-attention helps them \"get\" context way better than older AI\n",
      "- **Faster training**: Parallelization means they can learn from massive amounts of text quickly\n",
      "- **Flexible**: The same architecture works for translation, chatbots, code generation, and more\n",
      "\n",
      "Think of transformers as having given AI a major brain upgrade - like going from a flip phone to a smartphone! ğŸ§ âœ¨\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ¤– 3. deepseek/deepseek-v3.1-terminus\n",
      "--------------------------------------------------\n",
      "Of course! Let's break down Transformers like you're building a super-smart, super-fast translation machine.\n",
      "\n",
      "Imagine you have two friends: **Alex the Architect** and **Dana the Decoder**. They need to work together to translate a sentence from English to Spanish.\n",
      "\n",
      "### The Big Idea: Teamwork\n",
      "\n",
      "The **Transformer** is their teamwork system. It has two main parts:\n",
      "\n",
      "1.  **The Encoder (Alex the Architect):** His job is to *understand* the original sentence completely. He figures out what each word means and, most importantly, how each word relates to all the others.\n",
      "2.  **The Decoder (Dana the Decoder):** Her job is to *generate* the new Spanish sentence, one word at a time. But she doesn't guess blindly; she constantly checks her work against the \"blueprint\" that Alex created.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 1: Alex the Architect's Job (The Encoder)\n",
      "\n",
      "Alex doesn't just read the words in order. He's a genius who can look at the *entire sentence at once* and see the connections between all the words.\n",
      "\n",
      "This superpower is called **Self-Attention**.\n",
      "\n",
      "**What is Self-Attention?**\n",
      "Think of the sentence: **\"The cat sat on the mat because it was tired.\"**\n",
      "\n",
      "What does \"**it**\" refer to? The cat, right? A simple computer might get confused, but Alex uses self-attention. He asks: \"For the word 'it', which other word in this sentence is most important?\" He quickly checks all the other words and realizes \"it\" is strongly connected to \"cat.\" He understands the context!\n",
      "\n",
      "**How he does it (the simple version):**\n",
      "1.  He gives each word a little \"name tag\" that says what it is (a noun, verb, etc.).\n",
      "2.  Then, he throws a party for all the words. Each word mingles with every other word, and they figure out who they're most related to. \"Cat\" and \"sat\" become friends. \"It\" and \"cat\" become best friends. \"Tired\" and \"cat\" also connect.\n",
      "3.  He creates a new, \"enriched\" version of each word that now contains information about all its friends in the sentence. This enriched version is the **blueprint**.\n",
      "\n",
      "Alex does this for every word in the sentence, all at the same time! This is called **Parallelization**. It's like having multiple Alexes working on different words simultaneously, which makes the process incredibly fast.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 2: Dana the Decoder's Job (The Decoder)\n",
      "\n",
      "Now Dana has to build the Spanish sentence. She works one word at a time, but she's also a genius in her own way.\n",
      "\n",
      "1.  **She Starts Simple:** She begins with a \"start\" signal.\n",
      "2.  **She Uses Her Own Self-Attention:** She looks at the words she's already written (so far, just \"start\") to make sure she's building a sensible sentence. She can't peek at future words she hasn't written yet!\n",
      "3.  **The Key Step - Checking the Blueprint:** Before writing the next word, she stops and asks Alex: \"Hey, based on your blueprint of the English sentence, what should the next Spanish word be?\"\n",
      "\n",
      "She compares her incomplete Spanish sentence with Alex's enriched English blueprint. This comparison is the magic link between the encoder and decoder.\n",
      "\n",
      "4.  **She Writes the Word:** Based on that comparison, she predicts the most likely next word (e.g., \"El\").\n",
      "5.  **Repeat:** She takes \"El\" and adds it to her sentence. Then she goes back to step 2, using \"Start, El\" to predict the next word (\"gato\"), and so on, until she writes an \"end\" signal.\n",
      "\n",
      "---\n",
      "\n",
      "### Real-World Analogy: Building with LEGOs\n",
      "\n",
      "*   **Encoder (Alex):** You get a picture of a finished LEGO spaceship (the original sentence). You analyze the picture, noticing how the wings connect to the body and where the cockpit goes. You create a perfect building plan in your head. This is the **blueprint**.\n",
      "*   **Decoder (Dana):** You have a box of LEGOs (words in Spanish). You start building, one brick at a time. Before placing each new brick, you look at your partial build and then check the picture in your head (the blueprint) to choose the absolutely perfect next piece.\n",
      "\n",
      "### Why is this a Big Deal?\n",
      "\n",
      "1.  **Parallelization:** The encoder can analyze all words at once, unlike older systems that read words one by one. This makes it lightning-fast to train on powerful computers.\n",
      "2.  **Self-Attention:** It understands context perfectly. It knows that \"bank\" in \"river bank\" is different from \"bank\" in \"bank account.\"\n",
      "\n",
      "This architecture is so powerful that it's not just for translation. It's the brain behind **ChatGPT, Google Search, and more!** It's the foundation for most modern AI that understands and generates language.\n",
      "\n",
      "**So, in a nutshell:** The Transformer is a team where one part (Encoder) deeply understands the input, and the other part (Decoder) uses that understanding to generate the output, with both parts using a special power (Self-Attention) to see how all the pieces fit together, all done at an incredible speed (Parallelization).\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ¤– 4. moonshotai/kimi-k2-0905\n",
      "--------------------------------------------------\n",
      "Imagine you and your best friend want to send secret messages across the classroom, but you can only pass folded notes.  \n",
      "The â€œTransformerâ€ is the super-smart note-passing system that lets you do it faster and better than any old way.\n",
      "\n",
      "1. The big picture  \n",
      "   â€“ There are two main Lego boxes:  \n",
      "     â€¢ Encoder: the â€œwriterâ€ box. It reads your whole message at once and packs the meaning into a new note.  \n",
      "     â€¢ Decoder: the â€œreaderâ€ box. It takes that packed meaning and writes the reply, one word at a time.\n",
      "\n",
      "2. Self-attention (the magic trick)  \n",
      "   â€“ While the encoder is reading, every word looks around at all the other words in the sentence and asks, â€œWho is important to me right now?â€  \n",
      "   â€“ Example: in â€œThe cat sat on the mat,â€ when the word â€œsatâ€ is packing its meaning, it gives high-five stickers to â€œcatâ€ and â€œmatâ€ because they matter most.  \n",
      "   â€“ This happens for every word at the same timeâ€”no need to wait.\n",
      "\n",
      "3. Parallelization (the speed boost)  \n",
      "   â€“ Old systems read words one-by-one like a slow audiobook.  \n",
      "   â€“ Transformers spread the words out like a group project: every word works on its self-attention homework simultaneously.  \n",
      "   â€“ Result: the whole sentence is done in one big snapshot instead of a slow chain.\n",
      "\n",
      "4. Decoder side  \n",
      "   â€“ When writing the reply, the decoder uses two kinds of attention:  \n",
      "     a) Self-attention inside its own reply-so-far.  \n",
      "     b) â€œEncoder-decoderâ€ attention that peeks back at the packed meaning note it got from the encoder.  \n",
      "   â€“ It predicts the next word, adds it to the reply, and repeats until it writes an end-token (like â€œDone!â€).\n",
      "\n",
      "5. Quick recap  \n",
      "   Encoder: â€œI see everything at once and zip the meaning into a package.â€  \n",
      "   Decoder: â€œI look at that package and my own reply-so-far, then guess the next word.â€  \n",
      "   Self-attention: â€œEvery word chats with every other word to see whoâ€™s important.â€  \n",
      "   Parallelization: â€œAll words work together so no one waits in line.â€\n",
      "\n",
      "Thatâ€™s the whole transformer: a super-fast, group-chat note system that turns your sentence into meaning and then into a replyâ€”no slow single-file lines needed.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Extract full response content for a specific prompt\n",
    "prompt_id = \"1b7442da-f930-42a3-ab8b-4a7d457ee648\"  # Replace with your prompt ID\n",
    "\n",
    "info = inspector.get_prompt_info(prompt_id=prompt_id)\n",
    "if info and info['responses']:\n",
    "    prompt_text = info['prompt']['prompt_text']\n",
    "    print(f\"ğŸ“ PROMPT: {prompt_text}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, response in enumerate(info['responses'], 1):\n",
    "        if response['response_content'] and not response['response_error']:\n",
    "            print(f\"\\nğŸ¤– {i}. {response['model_name']}\")\n",
    "            print(\"-\" * 50)\n",
    "            print(response['response_content'])\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "        else:\n",
    "            print(f\"\\nğŸ¤– {i}. {response['model_name']} - âŒ Error: {response['response_error']}\")\n",
    "else:\n",
    "    print(\"âŒ No responses found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Quick Access Functions\n",
    "\n",
    "Use these cells for quick access to common operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick function to inspect any prompt ID\n",
    "def quick_inspect(prompt_id):\n",
    "    info = inspector.get_prompt_info(prompt_id=prompt_id)\n",
    "    inspector.display_prompt_info(info)\n",
    "    return info\n",
    "\n",
    "# Example usage:\n",
    "# quick_inspect(\"your-prompt-id-here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick function to get just the response content\n",
    "def get_response_content(prompt_id, model_name=None):\n",
    "    info = inspector.get_prompt_info(prompt_id=prompt_id)\n",
    "    if not info or not info['responses']:\n",
    "        return None\n",
    "    \n",
    "    if model_name:\n",
    "        # Get specific model response\n",
    "        for response in info['responses']:\n",
    "            if response['model_name'] == model_name:\n",
    "                return response['response_content']\n",
    "        return f\"Model '{model_name}' not found\"\n",
    "    else:\n",
    "        # Get all responses\n",
    "        responses = {}\n",
    "        for response in info['responses']:\n",
    "            responses[response['model_name']] = response['response_content']\n",
    "        return responses\n",
    "\n",
    "# Example usage:\n",
    "# content = get_response_content(\"your-prompt-id\", \"openai/gpt-5\")\n",
    "# all_content = get_response_content(\"your-prompt-id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  LLM Evaluation Engine\n",
    "\n",
    "Evaluate model responses using Grok-4-Fast as a judge with the evaluation system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM Evaluator initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satvikp/Desktop/educhain_build/edu/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3553: UserWarning: WARNING! headers is not default parameter.\n",
      "                headers was transferred to model_kwargs.\n",
      "                Please confirm that headers is what you intended.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class LLMEvaluator:\n",
    "    def __init__(self):\n",
    "        self.api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Missing OPENROUTER_API_KEY environment variable\")\n",
    "        \n",
    "        # Initialize Grok-4-Fast as judge model\n",
    "        self.judge_model = ChatOpenAI(\n",
    "            model=\"x-ai/grok-4-fast:free\",\n",
    "            openai_api_key=self.api_key,\n",
    "            openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "            headers={\n",
    "                \"HTTP-Referer\": \"https://github.com/satvik314/educhain-tutorbench\",\n",
    "                \"X-Title\": \"Educhain TutorBench Evaluator\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Load evaluation system prompt\n",
    "        self.system_prompt = self._load_evaluation_prompt()\n",
    "    \n",
    "    def _load_evaluation_prompt(self):\n",
    "        \"\"\"Load the evaluation system prompt from file\"\"\"\n",
    "        try:\n",
    "            with open('/Users/satvikp/Desktop/mygit/educhain-tutorbench/prompts/evaluation_prompt.md', 'r') as f:\n",
    "                return f.read()\n",
    "        except FileNotFoundError:\n",
    "            print(\"âš ï¸ evaluation_prompt.md not found. Using default evaluation prompt.\")\n",
    "            return \"\"\"\n",
    "You are an expert educational assessment specialist. Evaluate AI tutoring responses based on teaching effectiveness.\n",
    "\n",
    "Score each response on these dimensions (1-10):\n",
    "1. Confusion Recognition: How well does it identify student's confusion?\n",
    "2. Adaptive Response: How well matched to student's level and needs?\n",
    "3. Learning Facilitation: Will the student understand and learn?\n",
    "4. Strategic Decision-Making: Good choice of teaching approach?\n",
    "5. Engagement & Emotional Intelligence: Appropriate tone and motivation?\n",
    "\n",
    "Provide scores and brief explanations for each dimension.\n",
    "\"\"\"\n",
    "    \n",
    "    def evaluate_single_response(self, prompt_text, model_name, response_content):\n",
    "        \"\"\"Evaluate a single model response\"\"\"\n",
    "        evaluation_prompt = f\"\"\"\n",
    "{self.system_prompt}\n",
    "\n",
    "## Evaluation Task\n",
    "\n",
    "**Teaching Scenario/Student Question:**\n",
    "{prompt_text}\n",
    "\n",
    "**AI Model Response ({model_name}):**\n",
    "{response_content}\n",
    "\n",
    "Please evaluate this response using the framework provided. Provide scores for all 5 dimensions and follow the output format specified.\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.judge_model.invoke(evaluation_prompt)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error during evaluation: {str(e)}\"\n",
    "    \n",
    "    def evaluate_multiple_responses(self, prompt_text, responses):\n",
    "        \"\"\"Evaluate multiple responses comparatively\"\"\"\n",
    "        response_text = \"\"\n",
    "        for i, resp in enumerate(responses, 1):\n",
    "            response_text += f\"\\n**Response {chr(64+i)} ({resp['model_name']}):**\\n{resp['response_content']}\\n\"\n",
    "        \n",
    "        evaluation_prompt = f\"\"\"\n",
    "{self.system_prompt}\n",
    "\n",
    "## Comparative Evaluation Task\n",
    "\n",
    "**Teaching Scenario/Student Question:**\n",
    "{prompt_text}\n",
    "\n",
    "**AI Model Responses:**\n",
    "{response_text}\n",
    "\n",
    "Please evaluate these responses using the comparative evaluation framework. Rank them and provide detailed comparison.\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.judge_model.invoke(evaluation_prompt)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error during comparative evaluation: {str(e)}\"\n",
    "    \n",
    "    def parse_scores(self, evaluation_text):\n",
    "        \"\"\"Parse numerical scores from evaluation text\"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        # Multiple patterns to catch different formats\n",
    "        patterns = {\n",
    "            'confusion_recognition': [\n",
    "                r'Confusion Recognition:?\\s*(\\d+)/10',\n",
    "                r'Confusion Recognition.*?(\\d+)\\s*/\\s*10',\n",
    "                r'(?:^|\\n)\\s*-?\\s*Confusion Recognition:?\\s*(\\d+)/10'\n",
    "            ],\n",
    "            'adaptive_response': [\n",
    "                r'Adaptive Response:?\\s*(\\d+)/10',\n",
    "                r'Adaptive Response.*?(\\d+)\\s*/\\s*10',\n",
    "                r'(?:^|\\n)\\s*-?\\s*Adaptive Response:?\\s*(\\d+)/10'\n",
    "            ],\n",
    "            'learning_facilitation': [\n",
    "                r'Learning Facilitation:?\\s*(\\d+)/10',\n",
    "                r'Learning Facilitation.*?(\\d+)\\s*/\\s*10',\n",
    "                r'(?:^|\\n)\\s*-?\\s*Learning Facilitation:?\\s*(\\d+)/10'\n",
    "            ],\n",
    "            'strategic_decision': [\n",
    "                r'Strategic Decision(?:-Making)?:?\\s*(\\d+)/10',\n",
    "                r'Strategic Decision.*?(\\d+)\\s*/\\s*10',\n",
    "                r'(?:^|\\n)\\s*-?\\s*Strategic Decision(?:-Making)?:?\\s*(\\d+)/10'\n",
    "            ],\n",
    "            'engagement_eq': [\n",
    "                r'Engagement.*?(?:EQ|Intelligence):?\\s*(\\d+)/10',\n",
    "                r'Engagement.*?(?:EQ|Intelligence).*?(\\d+)\\s*/\\s*10',\n",
    "                r'(?:^|\\n)\\s*-?\\s*Engagement.*?(?:EQ|Intelligence):?\\s*(\\d+)/10'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for dimension, pattern_list in patterns.items():\n",
    "            for pattern in pattern_list:\n",
    "                match = re.search(pattern, evaluation_text, re.IGNORECASE | re.MULTILINE)\n",
    "                if match:\n",
    "                    scores[dimension] = int(match.group(1))\n",
    "                    break\n",
    "        \n",
    "        # Try to extract overall score\n",
    "        overall_patterns = [\n",
    "            r'Overall.*?(?:Effectiveness\\s+)?Score.*?(\\d+(?:\\.\\d+)?)/10',\n",
    "            r'Overall.*?Score.*?(\\d+(?:\\.\\d+)?)\\s*/\\s*10',\n",
    "            r'\\*\\*Overall.*?Score\\*\\*:?\\s*(\\d+(?:\\.\\d+)?)/10'\n",
    "        ]\n",
    "        \n",
    "        for pattern in overall_patterns:\n",
    "            match = re.search(pattern, evaluation_text, re.IGNORECASE | re.MULTILINE)\n",
    "            if match:\n",
    "                scores['overall'] = float(match.group(1))\n",
    "                break\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def store_evaluation(self, prompt_id, model_name, evaluation_text, scores):\n",
    "        \"\"\"Store evaluation results in database\"\"\"\n",
    "        evaluation_data = {\n",
    "            \"prompt_id\": prompt_id,\n",
    "            \"model_name\": model_name,\n",
    "            \"evaluation_text\": evaluation_text,\n",
    "            \"scores\": json.dumps(scores) if scores else None,\n",
    "            \"judge_model\": \"x-ai/grok-4-fast:free\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            result = inspector.supabase.table(\"llm_evaluations\").insert(evaluation_data).execute()\n",
    "            return result.data[0][\"id\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error storing evaluation: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize the evaluator\n",
    "evaluator = LLMEvaluator()\n",
    "print(\"âœ… LLM Evaluator initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Evaluate Individual Response\n",
    "\n",
    "Evaluate a single model response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_model(prompt_id, model_name, store_results=True):\n",
    "    \"\"\"Evaluate a single model response for a given prompt\"\"\"\n",
    "    \n",
    "    # Get prompt information\n",
    "    info = inspector.get_prompt_info(prompt_id=prompt_id)\n",
    "    if not info:\n",
    "        return None\n",
    "    \n",
    "    prompt_text = info['prompt']['prompt_text']\n",
    "    \n",
    "    # Find the specific model response\n",
    "    target_response = None\n",
    "    for response in info['responses']:\n",
    "        if response['model_name'] == model_name:\n",
    "            target_response = response\n",
    "            break\n",
    "    \n",
    "    if not target_response:\n",
    "        print(f\"âŒ Model '{model_name}' not found in responses\")\n",
    "        return None\n",
    "    \n",
    "    if target_response['response_error']:\n",
    "        print(f\"âŒ Cannot evaluate - model had error: {target_response['response_error']}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ğŸ” Evaluating {model_name} response...\")\n",
    "    print(f\"ğŸ“ Prompt: {prompt_text[:100]}...\")\n",
    "    print()\n",
    "    \n",
    "    # Perform evaluation\n",
    "    evaluation = evaluator.evaluate_single_response(\n",
    "        prompt_text, \n",
    "        model_name, \n",
    "        target_response['response_content']\n",
    "    )\n",
    "    \n",
    "    # Parse scores\n",
    "    scores = evaluator.parse_scores(evaluation)\n",
    "    \n",
    "    # Store in database if requested\n",
    "    eval_id = None\n",
    "    if store_results:\n",
    "        eval_id = evaluator.store_evaluation(prompt_id, model_name, evaluation, scores)\n",
    "        if eval_id:\n",
    "            print(f\"âœ… Evaluation stored in database (ID: {eval_id})\")\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ğŸ§  EVALUATION RESULTS - {model_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if scores:\n",
    "        print(\"ğŸ“Š SCORES:\")\n",
    "        for dimension, score in scores.items():\n",
    "            dimension_name = dimension.replace('_', ' ').title()\n",
    "            print(f\"   {dimension_name}: {score}/10\")\n",
    "        print()\n",
    "    \n",
    "    print(\"ğŸ“ DETAILED EVALUATION:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(evaluation)\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'evaluation': evaluation,\n",
    "        'scores': scores,\n",
    "        'eval_id': eval_id\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# result = evaluate_single_model(\"1b7442da-f930-42a3-ab8b-4a7d457ee648\", \"openai/gpt-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ† Evaluate All Responses (Comparative)\n",
    "\n",
    "Evaluate all model responses for a prompt comparatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_responses(prompt_id, store_results=True, include_comparative=True):\n",
    "    \"\"\"Evaluate all model responses for a given prompt\"\"\"\n",
    "    \n",
    "    # Get prompt information\n",
    "    info = inspector.get_prompt_info(prompt_id=prompt_id)\n",
    "    if not info:\n",
    "        return None\n",
    "    \n",
    "    prompt_text = info['prompt']['prompt_text']\n",
    "    responses = info['responses']\n",
    "    \n",
    "    # Filter out error responses\n",
    "    valid_responses = [r for r in responses if r['response_content'] and not r['response_error']]\n",
    "    \n",
    "    if not valid_responses:\n",
    "        print(\"âŒ No valid responses found to evaluate\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ğŸ” Evaluating {len(valid_responses)} model responses...\")\n",
    "    print(f\"ğŸ“ Prompt: {prompt_text[:100]}...\")\n",
    "    print()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Individual evaluations\n",
    "    print(\"ğŸ¤– INDIVIDUAL EVALUATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for response in valid_responses:\n",
    "        model_name = response['model_name']\n",
    "        print(f\"\\nğŸ“Š Evaluating {model_name}...\")\n",
    "        \n",
    "        # Perform evaluation\n",
    "        evaluation = evaluator.evaluate_single_response(\n",
    "            prompt_text, \n",
    "            model_name, \n",
    "            response['response_content']\n",
    "        )\n",
    "        \n",
    "        # Parse scores\n",
    "        scores = evaluator.parse_scores(evaluation)\n",
    "        \n",
    "        # Store in database if requested\n",
    "        eval_id = None\n",
    "        if store_results:\n",
    "            eval_id = evaluator.store_evaluation(prompt_id, model_name, evaluation, scores)\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'evaluation': evaluation,\n",
    "            'scores': scores,\n",
    "            'eval_id': eval_id\n",
    "        }\n",
    "        \n",
    "        # Show brief scores\n",
    "        if scores:\n",
    "            overall = scores.get('overall', 'N/A')\n",
    "            confusion = scores.get('confusion_recognition', 'N/A')\n",
    "            adaptive = scores.get('adaptive_response', 'N/A')\n",
    "            learning = scores.get('learning_facilitation', 'N/A')\n",
    "            print(f\"   Overall: {overall}/10 | Confusion: {confusion}/10 | Adaptive: {adaptive}/10 | Learning: {learning}/10\")\n",
    "        else:\n",
    "            print(\"   âš ï¸ Could not parse scores\")\n",
    "    \n",
    "    # Comparative evaluation\n",
    "    if include_comparative and len(valid_responses) > 1:\n",
    "        print(f\"\\nğŸ† COMPARATIVE EVALUATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        comparative_eval = evaluator.evaluate_multiple_responses(prompt_text, valid_responses)\n",
    "        results['comparative'] = comparative_eval\n",
    "        \n",
    "        print(comparative_eval)\n",
    "    \n",
    "    print(f\"\\nâœ… Evaluation completed!\")\n",
    "    if store_results:\n",
    "        print(f\"ğŸ’¾ Results stored in database\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# results = evaluate_all_responses(\"1b7442da-f930-42a3-ab8b-4a7d457ee648\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Evaluation Results Dashboard\n",
    "\n",
    "View evaluation results in a structured format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_evaluation_dashboard(prompt_id):\n",
    "    \"\"\"Display a comprehensive evaluation dashboard for a prompt\"\"\"\n",
    "    \n",
    "    # Get prompt information including existing evaluations\n",
    "    info = inspector.get_prompt_info(prompt_id=prompt_id)\n",
    "    if not info:\n",
    "        return None\n",
    "    \n",
    "    prompt_text = info['prompt']['prompt_text']\n",
    "    responses = info['responses']\n",
    "    evaluations = info['evaluations']\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(\"ğŸ§  EVALUATION DASHBOARD\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"ğŸ“ Prompt: {prompt_text}\")\n",
    "    print(f\"ğŸ¤– Models: {len(responses)} | ğŸ§  Evaluations: {len(evaluations)}\")\n",
    "    print()\n",
    "    \n",
    "    if not evaluations:\n",
    "        print(\"âŒ No evaluations found for this prompt.\")\n",
    "        print(\"ğŸ’¡ Use evaluate_single_model() or evaluate_all_responses() to generate evaluations.\")\n",
    "        return None\n",
    "    \n",
    "    # Create evaluation summary DataFrame\n",
    "    eval_data = []\n",
    "    for eval_item in evaluations:\n",
    "        scores = json.loads(eval_item['scores']) if eval_item['scores'] else {}\n",
    "        eval_data.append({\n",
    "            'Model': eval_item['model_name'],\n",
    "            'Overall': scores.get('overall', 'N/A'),\n",
    "            'Confusion Recognition': scores.get('confusion_recognition', 'N/A'),\n",
    "            'Adaptive Response': scores.get('adaptive_response', 'N/A'),\n",
    "            'Learning Facilitation': scores.get('learning_facilitation', 'N/A'),\n",
    "            'Strategic Decision': scores.get('strategic_decision', 'N/A'),\n",
    "            'Engagement & EQ': scores.get('engagement_eq', 'N/A'),\n",
    "            'Judge Model': eval_item['judge_model'],\n",
    "            'Evaluated': eval_item['created_at'][:10]  # Date only\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(eval_data)\n",
    "    \n",
    "    print(\"ğŸ“Š EVALUATION SCORES SUMMARY:\")\n",
    "    print(df.to_string(index=False))\n",
    "    print()\n",
    "    \n",
    "    # Show rankings\n",
    "    numeric_scores = df[df['Overall'] != 'N/A'].copy()\n",
    "    if not numeric_scores.empty:\n",
    "        numeric_scores['Overall'] = pd.to_numeric(numeric_scores['Overall'])\n",
    "        numeric_scores = numeric_scores.sort_values('Overall', ascending=False)\n",
    "        \n",
    "        print(\"ğŸ† RANKINGS (by Overall Score):\")\n",
    "        for i, (_, row) in enumerate(numeric_scores.iterrows(), 1):\n",
    "            medal = \"ğŸ¥‡\" if i == 1 else \"ğŸ¥ˆ\" if i == 2 else \"ğŸ¥‰\" if i == 3 else f\"{i}.\"\n",
    "            print(f\"   {medal} {row['Model']}: {row['Overall']}/10\")\n",
    "        print()\n",
    "    \n",
    "    # Show detailed evaluations option\n",
    "    print(\"ğŸ“ DETAILED EVALUATIONS:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for eval_item in evaluations:\n",
    "        print(f\"\\nğŸ¤– {eval_item['model_name']}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(eval_item['evaluation_text'])\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# dashboard = show_evaluation_dashboard(\"1b7442da-f930-42a3-ab8b-4a7d457ee648\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ Quick Evaluation Functions\n",
    "\n",
    "Ready-to-use functions for common evaluation tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Quick evaluation functions ready to use!\n"
     ]
    }
   ],
   "source": [
    "# Quick evaluation workflow functions\n",
    "\n",
    "def quick_evaluate(prompt_id):\n",
    "    \"\"\"Quick evaluation of all responses for a prompt\"\"\"\n",
    "    print(f\"ğŸš€ Starting quick evaluation for prompt: {prompt_id}\")\n",
    "    results = evaluate_all_responses(prompt_id, store_results=True, include_comparative=True)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    dashboard = show_evaluation_dashboard(prompt_id)\n",
    "    return results, dashboard\n",
    "\n",
    "def evaluate_by_prompt_text(prompt_text):\n",
    "    \"\"\"Find and evaluate the most recent prompt matching the text\"\"\"\n",
    "    info = inspector.get_prompt_info(prompt_text=prompt_text)\n",
    "    if info:\n",
    "        prompt_id = info['prompt']['id']\n",
    "        return quick_evaluate(prompt_id)\n",
    "    return None\n",
    "\n",
    "def compare_models(prompt_id, model_names):\n",
    "    \"\"\"Compare specific models for a prompt\"\"\"\n",
    "    results = {}\n",
    "    for model_name in model_names:\n",
    "        print(f\"\\nğŸ” Evaluating {model_name}...\")\n",
    "        result = evaluate_single_model(prompt_id, model_name, store_results=True)\n",
    "        if result:\n",
    "            results[model_name] = result\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    for model_name, result in results.items():\n",
    "        scores = result['scores']\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Overall': scores.get('overall', 'N/A'),\n",
    "            'Confusion Recognition': scores.get('confusion_recognition', 'N/A'),\n",
    "            'Adaptive Response': scores.get('adaptive_response', 'N/A'),\n",
    "            'Learning Facilitation': scores.get('learning_facilitation', 'N/A')\n",
    "        })\n",
    "    \n",
    "    if comparison_data:\n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        print(\"\\nğŸ“Š MODEL COMPARISON:\")\n",
    "        print(df.to_string(index=False))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example workflows:\n",
    "\n",
    "# 1. Quick evaluate all responses for a prompt\n",
    "# results, dashboard = quick_evaluate(\"1b7442da-f930-42a3-ab8b-4a7d457ee648\")\n",
    "\n",
    "# 2. Evaluate by prompt text\n",
    "# results = evaluate_by_prompt_text(\"Explain transformers architecture\")\n",
    "\n",
    "# 3. Compare specific models\n",
    "# comparison = compare_models(\"prompt-id-here\", [\"openai/gpt-5\", \"anthropic/claude-sonnet-4\"])\n",
    "\n",
    "print(\"âœ… Quick evaluation functions ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting quick evaluation for prompt: 1b7442da-f930-42a3-ab8b-4a7d457ee648\n",
      "ğŸ” Evaluating 4 model responses...\n",
      "ğŸ“ Prompt: Explain transformers architecture like I am 16. Use basic terms of architecture like encoder, decode...\n",
      "\n",
      "ğŸ¤– INDIVIDUAL EVALUATIONS\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Evaluating openai/gpt-5...\n",
      "   âš ï¸ Could not parse scores\n",
      "\n",
      "ğŸ“Š Evaluating anthropic/claude-sonnet-4...\n",
      "   âš ï¸ Could not parse scores\n",
      "\n",
      "ğŸ“Š Evaluating deepseek/deepseek-v3.1-terminus...\n",
      "   âš ï¸ Could not parse scores\n",
      "\n",
      "ğŸ“Š Evaluating moonshotai/kimi-k2-0905...\n",
      "   âš ï¸ Could not parse scores\n",
      "\n",
      "ğŸ† COMPARATIVE EVALUATION\n",
      "============================================================\n",
      "Error during comparative evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "\n",
      "âœ… Evaluation completed!\n",
      "ğŸ’¾ Results stored in database\n",
      "\n",
      "================================================================================\n",
      "====================================================================================================\n",
      "ğŸ§  EVALUATION DASHBOARD\n",
      "====================================================================================================\n",
      "ğŸ“ Prompt: Explain transformers architecture like I am 16. Use basic terms of architecture like encoder, decoder, self-attention, parallelization\n",
      "ğŸ¤– Models: 4 | ğŸ§  Evaluations: 12\n",
      "\n",
      "ğŸ“Š EVALUATION SCORES SUMMARY:\n",
      "                          Model Overall Confusion Recognition Adaptive Response Learning Facilitation Strategic Decision Engagement & EQ           Judge Model  Evaluated\n",
      "                   openai/gpt-5     N/A                   N/A               N/A                   N/A                N/A             N/A x-ai/grok-4-fast:free 2025-09-27\n",
      "      anthropic/claude-sonnet-4     N/A                   N/A               N/A                   N/A                N/A             N/A x-ai/grok-4-fast:free 2025-09-27\n",
      "deepseek/deepseek-v3.1-terminus     N/A                   N/A               N/A                   N/A                N/A             N/A x-ai/grok-4-fast:free 2025-09-27\n",
      "        moonshotai/kimi-k2-0905     N/A                   N/A               N/A                   N/A                N/A             N/A x-ai/grok-4-fast:free 2025-09-27\n",
      "                   openai/gpt-5     N/A                   N/A               N/A                   N/A                N/A             N/A x-ai/grok-4-fast:free 2025-09-27\n",
      "      anthropic/claude-sonnet-4     N/A                   N/A               N/A                   N/A                N/A             N/A x-ai/grok-4-fast:free 2025-09-27\n",
      "deepseek/deepseek-v3.1-terminus     N/A                   N/A               N/A                   N/A                N/A             N/A x-ai/grok-4-fast:free 2025-09-27\n",
      "        moonshotai/kimi-k2-0905     N/A                   N/A               N/A                   N/A                N/A             N/A x-ai/grok-4-fast:free 2025-09-27\n",
      "                   openai/gpt-5     N/A                   N/A               N/A                   N/A                N/A             N/A x-ai/grok-4-fast:free 2025-09-28\n",
      "      anthropic/claude-sonnet-4     N/A                   N/A               N/A                   N/A                N/A             N/A x-ai/grok-4-fast:free 2025-09-28\n",
      "deepseek/deepseek-v3.1-terminus     N/A                   N/A               N/A                   N/A                N/A             N/A x-ai/grok-4-fast:free 2025-09-28\n",
      "        moonshotai/kimi-k2-0905     N/A                   N/A               N/A                   N/A                N/A             N/A x-ai/grok-4-fast:free 2025-09-28\n",
      "\n",
      "ğŸ“ DETAILED EVALUATIONS:\n",
      "================================================================================\n",
      "\n",
      "ğŸ¤– openai/gpt-5\n",
      "--------------------------------------------------\n",
      "Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ¤– anthropic/claude-sonnet-4\n",
      "--------------------------------------------------\n",
      "Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ¤– deepseek/deepseek-v3.1-terminus\n",
      "--------------------------------------------------\n",
      "Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ¤– moonshotai/kimi-k2-0905\n",
      "--------------------------------------------------\n",
      "Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ¤– openai/gpt-5\n",
      "--------------------------------------------------\n",
      "Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ¤– anthropic/claude-sonnet-4\n",
      "--------------------------------------------------\n",
      "Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ¤– deepseek/deepseek-v3.1-terminus\n",
      "--------------------------------------------------\n",
      "Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ¤– moonshotai/kimi-k2-0905\n",
      "--------------------------------------------------\n",
      "Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ¤– openai/gpt-5\n",
      "--------------------------------------------------\n",
      "Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ¤– anthropic/claude-sonnet-4\n",
      "--------------------------------------------------\n",
      "Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ¤– deepseek/deepseek-v3.1-terminus\n",
      "--------------------------------------------------\n",
      "Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ¤– moonshotai/kimi-k2-0905\n",
      "--------------------------------------------------\n",
      "Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'openai/gpt-5': {'evaluation': \"Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\",\n",
       "   'scores': {},\n",
       "   'eval_id': '0a5621fd-4559-48e8-8416-776b8214bc72'},\n",
       "  'anthropic/claude-sonnet-4': {'evaluation': \"Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\",\n",
       "   'scores': {},\n",
       "   'eval_id': 'acff79c5-659d-4853-8236-df6d85581a4c'},\n",
       "  'deepseek/deepseek-v3.1-terminus': {'evaluation': \"Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\",\n",
       "   'scores': {},\n",
       "   'eval_id': '8cecc1d7-781a-440c-957e-9e40df99364c'},\n",
       "  'moonshotai/kimi-k2-0905': {'evaluation': \"Error during evaluation: Completions.create() got an unexpected keyword argument 'headers'\",\n",
       "   'scores': {},\n",
       "   'eval_id': 'df96ff16-1c9a-4728-84fa-697c95fc5b2c'},\n",
       "  'comparative': \"Error during comparative evaluation: Completions.create() got an unexpected keyword argument 'headers'\"},\n",
       "                               Model Overall Confusion Recognition  \\\n",
       " 0                      openai/gpt-5     N/A                   N/A   \n",
       " 1         anthropic/claude-sonnet-4     N/A                   N/A   \n",
       " 2   deepseek/deepseek-v3.1-terminus     N/A                   N/A   \n",
       " 3           moonshotai/kimi-k2-0905     N/A                   N/A   \n",
       " 4                      openai/gpt-5     N/A                   N/A   \n",
       " 5         anthropic/claude-sonnet-4     N/A                   N/A   \n",
       " 6   deepseek/deepseek-v3.1-terminus     N/A                   N/A   \n",
       " 7           moonshotai/kimi-k2-0905     N/A                   N/A   \n",
       " 8                      openai/gpt-5     N/A                   N/A   \n",
       " 9         anthropic/claude-sonnet-4     N/A                   N/A   \n",
       " 10  deepseek/deepseek-v3.1-terminus     N/A                   N/A   \n",
       " 11          moonshotai/kimi-k2-0905     N/A                   N/A   \n",
       " \n",
       "    Adaptive Response Learning Facilitation Strategic Decision Engagement & EQ  \\\n",
       " 0                N/A                   N/A                N/A             N/A   \n",
       " 1                N/A                   N/A                N/A             N/A   \n",
       " 2                N/A                   N/A                N/A             N/A   \n",
       " 3                N/A                   N/A                N/A             N/A   \n",
       " 4                N/A                   N/A                N/A             N/A   \n",
       " 5                N/A                   N/A                N/A             N/A   \n",
       " 6                N/A                   N/A                N/A             N/A   \n",
       " 7                N/A                   N/A                N/A             N/A   \n",
       " 8                N/A                   N/A                N/A             N/A   \n",
       " 9                N/A                   N/A                N/A             N/A   \n",
       " 10               N/A                   N/A                N/A             N/A   \n",
       " 11               N/A                   N/A                N/A             N/A   \n",
       " \n",
       "               Judge Model   Evaluated  \n",
       " 0   x-ai/grok-4-fast:free  2025-09-27  \n",
       " 1   x-ai/grok-4-fast:free  2025-09-27  \n",
       " 2   x-ai/grok-4-fast:free  2025-09-27  \n",
       " 3   x-ai/grok-4-fast:free  2025-09-27  \n",
       " 4   x-ai/grok-4-fast:free  2025-09-27  \n",
       " 5   x-ai/grok-4-fast:free  2025-09-27  \n",
       " 6   x-ai/grok-4-fast:free  2025-09-27  \n",
       " 7   x-ai/grok-4-fast:free  2025-09-27  \n",
       " 8   x-ai/grok-4-fast:free  2025-09-28  \n",
       " 9   x-ai/grok-4-fast:free  2025-09-28  \n",
       " 10  x-ai/grok-4-fast:free  2025-09-28  \n",
       " 11  x-ai/grok-4-fast:free  2025-09-28  )"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_evaluate(\"1b7442da-f930-42a3-ab8b-4a7d457ee648\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
